{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Decision Trees\n",
    "Author: Hovanes Gasparian\n",
    "\n",
    "*Some materials adapted from Matt Brems and David Yerrington from General Assembly and Victor Zhou from Facebook*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives\n",
    "\n",
    "- Explain how a decision tree functions\n",
    "- Learn how to build a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What will we wear today?\n",
    "\n",
    "|$Y = $ Clothing|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|   Coat  |      Rainy     |   Weekday  |\n",
    "|   T-Shirt   |      Sunny     |   Weekday  |\n",
    "|   Coat  |      Rainy     |   Weekend  |\n",
    "|  Hoodie  |      Sunny     |   Weekend  |\n",
    "|   Coat  |      Rainy     |   Weekday  |\n",
    "|  Hoodie  |      Sunny     |   Weekend  |\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>It's a rainy day. Based on our past, what do you think we'll wear?</summary>\n",
    "\n",
    "- A coat.\n",
    "- In 100% of past cases where the weather is rainy, we've worn a coat!\n",
    "\n",
    "|$Y = $ Clothing|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|   Coat  |      Rainy     |   Weekday  |\n",
    "|   Coat  |      Rainy     |   Weekend  |\n",
    "|   Coat  |      Rainy     |   Weekday  |\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>It's a sunny day. Based on our past, what do you think we'll wear?</summary>\n",
    "\n",
    "- Either a T-Shirt or a Hoodie... Based on our past, we've worn T-Shirts on 1/3 of sunny days and we wore Hoodies on 2/3 of sunny days.\n",
    "- If we **had** to make an educated guess, we'd probably go with Hoodie, because that's more likely given just this information.\n",
    "\n",
    "|$Y = $ Clothing|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|   T-Shirt   |      Sunny     |   Weekday  |\n",
    "|  Hoodie  |      Sunny     |   Weekend  |\n",
    "|  Hoodie  |      Sunny     |   Weekend  |\n",
    "\n",
    "</details>\n",
    "\n",
    "---\n",
    "\n",
    "<details><summary>It's a sunny day that also happens to be a weekend. Based on our past, what do you think we'll wear?</summary>\n",
    "\n",
    "- A Hoodie.\n",
    "- In 100% of past cases where the weather is sunny and where it's a weekend, we've worn a hoodie!\n",
    "\n",
    "|$Y = $ Clothing|$X_1 = $ Weather|$X_2 = $ Day|\n",
    "|:---------:|:--------------:|:----------:|\n",
    "|  Hoodie  |      Sunny     |   Weekend  |\n",
    "|  Hoodie  |      Sunny     |   Weekend  |\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees: Overview\n",
    "\n",
    "A decision tree:\n",
    "- takes a dataset consisting of $X$ and $Y$ data, \n",
    "- finds rules based on our $X$ data that splits the data into smaller subsets so that at the end, the values $Y$ in each \"leaf node\" are as \"pure\" as possible.\n",
    "\n",
    "Decision trees are often represented by a graph.\n",
    "\n",
    "<img src=\"./DecisionTree.png\" alt=\"what_to_wear\" width=\"850\"/>\n",
    "\n",
    "- (This image was created using [Draw.io](https://www.draw.io/).)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "Decision trees are aptly named, because they look like upside down trees. \n",
    "- What you see on top is known as the \"root node,\" through which all observations are passed.\n",
    "- At each internal split, the dataset is partitioned.\n",
    "- At each of the \"leaf nodes\" (i.e. the clothing boxes), we have a partition of observations that are as pure as possible.\n",
    "    - In this clothing example, each leaf node is perfectly pure. Once we get to a leaf node, every observation in that leaf node has the exact same value of $Y$!\n",
    "    - There are ways to quantify this idea of \"purity,\" and how the underlying algorithm decides where to make splits, which is demonstrated in the handout I gave you in class.\n",
    "\n",
    "For your reference, decision trees are technically referred to as \"**Classification and Regression Trees**,\" abbreviated as \"**CART**.\"\n",
    "- [DecisionTreeClassifier Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "- [DecisionTreeRegressor Documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purity in Decision Trees\n",
    "\n",
    "When quantifying how \"pure\" a node is, we want to see what the distribution of $Y$ is in each node, then summarize this distribution with a number.\n",
    "\n",
    "- For continuous $Y$ (i.e. using a decision tree to predict income), the default option is mean squared error.\n",
    "    - When the decision tree is figuring out which split to make at a given node, it picks the split that minimizes the MSE at that step.\n",
    "    \n",
    "    \n",
    "- For discrete $Y$, the default option is the Gini impurity. (This is not quite the same thing as the [Gini coefficient](https://en.wikipedia.org/wiki/Gini_coefficient).)\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "\\text{Gini} &=& 1 - \\sum_{i=1}^{classes} P(\\text{class i})^2 \\\\\n",
    "\\text{Gini (2 classes)} &=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 \\\\\n",
    "\\text{Gini (3 classes)} &=& 1 - P(\\text{class 1})^2 - P(\\text{class 2})^2 - P(\\text{class 3})^2 \\\\\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "- We use Gini to measure how pure a split is.\n",
    "- Gini impurity ranges from 0 to 0.5  when you have 2 classes (binary case), where:\n",
    "    - 0 means \"most pure.\"\n",
    "    - 0.5 means \"least pure.\"\n",
    "    \n",
    "- Gini impurity (with $k$ classes) ranges from 0 to $1-\\frac{1}{k}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Y variable from the previous clothing example\n",
    "Y = ['Coat', 'T-Shirt', 'Coat', 'Hoodie', 'Coat', 'Hoodie']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Gini function.\n",
    "def gini(y):\n",
    "    # create an empty list to store squared probabilities\n",
    "    gini_sum = []\n",
    "\n",
    "    # iterate through my classes, without double-counting\n",
    "    for i in set(y):\n",
    "        # calculate probability\n",
    "        prob = y.count(i) / len(y)\n",
    "        \n",
    "        # append squared proability\n",
    "        gini_sum.append(prob**2)\n",
    "    \n",
    "    # return Gini impurity\n",
    "    return 1 - sum(gini_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6111111111111112"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check gini impurity of current Y variable\n",
    "gini(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Under what circumstances should our Gini be 0?</summary>\n",
    "- When all of our observations are identical.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Under what circumstances should our Gini be 0?\n",
    "gini(['Coat', 'Coat', 'Coat'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>Under what circumstances should our Gini be 0.5?</summary>\n",
    "- When we have exactly two outcomes that are equally represented.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Under what circumstances should our Gini be 0.5?\n",
    "gini(['T-Shirt', 'Hoodie'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do decision trees use Gini to decide where to make a split?\n",
    "\n",
    "- At any node, it considers the subset of observations that exist at that node.\n",
    "- It iterates through each variable that could potentially split the data.\n",
    "- It then calculates the Gini impurity that results from every split.\n",
    "- Finally, it selects the variable that causes the greatest decrease in Gini impurity from the parent node to the child node.\n",
    "\n",
    "Optimization occurs at each node individually, making this a **greedy** algorithm. This is one of the disadvantages that will be discussed more at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read in Breast Cancer data.\n",
    "bcdf = pd.read_csv('./breast-cancer.csv', header=None)\n",
    "\n",
    "# create column names from header file\n",
    "field_names = open('./field_names.txt').read().splitlines()\n",
    "\n",
    "# set column names as field_names\n",
    "bcdf.columns = field_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outcome variable for malignant tumors\n",
    "bcdf['malignant'] = pd.get_dummies(bcdf.diagnosis)['M']\n",
    "y = bcdf.malignant\n",
    "\n",
    "# Create independent variables\n",
    "X = bcdf.drop(columns=['diagnosis', 'ID', 'malignant'])\n",
    "\n",
    "# Conduct train/test split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import decision tree model\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model\n",
    "dt = DecisionTreeClassifier(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort=False,\n",
       "                       random_state=42, splitter='best')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model.\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model.\n",
    "print(f'Training Score: {dt.score(X_train, y_train)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Score: 0.9415204678362573\n"
     ]
    }
   ],
   "source": [
    "print(f'Testing Score: {dt.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What conclusion do we usually draw when we see this kind of result?</summary>\n",
    "\n",
    "- The model is overfit to the data.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary>What might we do to alleviate this situation?</summary>\n",
    "    \n",
    "- Try to get more data.\n",
    "- Remove some features.\n",
    "- Tune your hyperparameters\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "## Advantages\n",
    "\n",
    "### 1. No need to standardize\n",
    " - Decision trees don't rely on scale to be effective.\n",
    "\n",
    "### 2. Decision trees don't care about how your data is distributed.\n",
    "\n",
    " - Decision trees are nonparametric, meaning we don't make assumptions about how our data or errors are distributed. Thus, they are effective even if your data is heavily skewed or not normally distributed. \n",
    "\n",
    "### 3. Easy to interpret.\n",
    "\n",
    " - The output of a decision tree is easy to interpet and and there is even a built-in method for showing feature importance\n",
    "\n",
    "### 4. Speed.\n",
    "\n",
    " - Decision tree models are very fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disadvantages\n",
    "\n",
    "\n",
    "### 1. Decision trees are prone to overfitting.\n",
    " - But we already discussed some ways to mitigate that, and there are others.\n",
    "\n",
    "### 2. Decision trees are locally optimal.\n",
    " - Because we're making the best decision at each local node (i.e. greedy search), we might end up with a less than optimal outcome.\n",
    "\n",
    "### 3. Decision trees don't work well with unbalanced data.\n",
    " - As with most classification methods, decision trees also bias results toward the majority class. There are workarounds for this too that we will discuss in future lessons!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "state": {
    "0b28c6b3b13649718658d43e965c8062": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
